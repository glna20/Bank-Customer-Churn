# -*- coding: utf-8 -*-
"""Customer Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11hJ3HnuBR4gpynnJAUiJdm2nCYZFA83L

**BANK CUSTOMER CHURN PREDICTION - PROJECT INTRODUCTION**

Customer churn simply refers to where a customer stops doing business with a company. In banking, this usually means that a customer closes their account or stops using the bank's services.


Banks invest heavily in acquiring these customers, and losing these customers is not ideal especially profitable ones as it can significantly impact the business revenue.

Churn Prediction becomes vital because it aids retaining a customer which is cheaper than acquiring a new one, it helps identify customers likely to churn which banks can take retention measures like offering incentives or improving customer service.

This project aims to build a supervised machine learning predictive model that can classify whether a customer is likely to churn (leave) or stay, based on their attributes and activity history.

The dataset is sourced from Kaggle and contains information about bank customers, including:

* Demographic info (age, gender, geography)

* Account details (balance, number of products, credit score)

* Customer behavior (active status, tenure)

* Target variable: Exited (1 = churned, 0 = stayed)

**1. Importing the libraries and dependencies**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import precision_recall_curve, average_precision_score
from sklearn.metrics import roc_curve, roc_auc_score

""" **2. Data Loading and Cleaning**"""

# loading the csv data to a pandas dataframe
df = pd.read_csv('Churn_Modelling.csv')

df.head()

df.shape

"""The dataset contains 10002 rows and 14 columns including 1 target Churn Status column."""

df.info()

"""**Key Observations:**

* The dataset has 10002 rows and 14 columns.
* It contains both numerical and categorical features.
* There are missing values in Geography, Age, HasCrCard and
  IsActivMember field.
* The target variable for churn prediction is "Exited".

***a. check the unique values in each variable.***

I'm not going to check the unique features in RowID, CustomerID and Surname, as they have lots of unique values.
"""

for col in df.columns[3:]:
  print(col, df[col].unique())
  print(col, df[col].unique().size)
  print('-'*50)

"""***b. Check and impute the missing values***"""

df[df.isnull().any(axis= 1)]

"""Using the mean to replace the missing value in the Age field, and the mode for the other fields.

"""

df['Age'].fillna(df['Age'].mean(), inplace= True)
df['Geography'].fillna(df['Geography'].mode()[0], inplace= True)
df['HasCrCard'].fillna(df['HasCrCard'].mode()[0], inplace= True)
df['IsActiveMember'].fillna(df['IsActiveMember'].mode()[0], inplace= True)

df[df.isnull().any(axis=1)]

"""***c. Check for duplicates***"""

df[df.duplicated()]

"""Delete the duplicates found"""

df = df.drop_duplicates(subset = 'CustomerId', keep= 'first')

df.head()

"""***d. Changing the data type of Age, HasCrCard and IsActiveMember from float to integers***"""

df['HasCrCard'] = df['HasCrCard'].astype(int)
df['IsActiveMember'] = df['IsActiveMember'].astype(int)
df.head()

df.dtypes

"""Dropping RowNumber, CustomerID and Surname columns as they are not useful for modelling"""

df = df.drop(columns= ['RowNumber', 'CustomerId', 'Surname'])
df.head()

df.shape

"""3. **Exploratory Data Analysis (EDA)**"""

df.describe()

"""**Key Observations:**

* Customers are between 18 - 92 years old and Credit Score varies from 350 to 850, with an average of 650.
* Each Customer has at least one bank product they use, and the max being 4.
* Customer salaries range between 11.58 and 199,992, with an average of 100,090.
* The number of years the customer has been with the bank is between 0 - 10 years.

***a. Analysis on Numerical Features***

To check and understand the distribution of the numerical variables
"""

num_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']

for col in num_cols:

  plt.figure(figsize= (5,3))
  sns.histplot(data= df, x= col, kde= True)
  plt.title(f'Distribution of {col}')

  # also want to calculate the mean and median
  df_mean = df[col].mean()
  df_median = df[col].median()

  # add vertical lines for mean and median
  plt.axvline(df_mean, color= 'red', linestyle= 'dashed', label = 'Mean')
  plt.axvline(df_median, color= 'green', linestyle= 'solid', label = 'Median')

  # add legends
  plt.legend()
  plt.show()

"""**The histogram provides insights into the distribution of numerical variables:**

* **Right-Skewed Distribution:** Age exhibits a righ-skewness which might be caused by the smaller proportion of older customers. The Number of Products  exhibit a more discrete distribution, likely due to predefined rating scales, and show a concentration towards a certain rate .

* **Uniform-Like Distribution:** Features such as Tenure and Estimated Salary appear to have a relatively uniform (even and consistent) spread rather than forming a normal distribution. Credit Score appears to be approximately normal, with a slight right skewness indidcating a healthy customer base

* **Bimodal:** The Customer balance shows two customer groups, one with zero balance and another with substantial balance.

Check for outliers in the numerical features
"""

num_cols = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']

for col in num_cols:

  plt.figure(figsize= (5,3))
  sns.boxplot(data = df, y= df[col])
  plt.title(f'Boxplot of {col}')
  plt.ylabel(col)
  plt.show()

def find_outliers_IQR(df):

   Q1 = df.quantile(0.25)

   Q3 = df.quantile(0.75)

   IQR = Q3 - Q1

   # Define lower and upper bounds using IQR (interquartile range)
   lower_bound = Q1 -( 1.5 * IQR)
   upper_bound = Q3 + (1.5 * IQR)

   # Filter and show outlier values of Age
   outliers = df[((df< lower_bound) | (df> upper_bound))]
   outlier_count = outliers.shape[0]
   outlier_percentage = 100 * outlier_count / df.shape[0]


   print(f'{[col]}: Q1 = {Q1}, Q3 = {Q3}, IQR = {IQR}, lb = {lower_bound}, up = {upper_bound}')
   print(f'{[col]}: has {outlier_count} (outliers {round(outlier_percentage, 2)}%)')
   return outliers

for col in df[['CreditScore', 'Age', 'NumOfProducts']]:
  outliers = find_outliers_IQR(df[col])

  print(f'max outlier value: {outliers.max()}')
  print(f'min outlier value: {outliers.min()}')
  print('-' *50)

  #print(outliers.sort_values())

"""The outliers are not relatively high, i can either drop or replace them. I will replace the Age with the median and ignore the outliers in the CreditScore and NumofProducts fields just to keep a decent number of data for modelling."""

# Replacing the outlier with the median

def impute_outliers_IQR(df):

   q1=df.quantile(0.25)

   q3=df.quantile(0.75)

   IQR=q3-q1

   upper = df[~(df>(q3+1.5*IQR))].max()

   lower = df[~(df<(q1-1.5*IQR))].min()

   df = np.where(df > upper,

       df.median(),

       np.where(

           df < lower,

           df.median(),

           df

           )

       )

   return df

df['Age'] = impute_outliers_IQR(df['Age'])

df.describe()

"""Convert Age datatype from float to integer"""

df['Age'] = df['Age'].astype(int)
df.dtypes

"""Heatmap to get correlation between the numerical features. It helps identify relationships or patterns between features that might influence the model or highlight redundant information."""

# heatmap to get correlation between the numerical features
plt.figure(figsize= (10,8))
sns.heatmap(df[num_cols].corr(), annot= True, cmap = 'coolwarm', fmt = '.2f')
plt.title('Correlation Heatmap between Numerical Features')
plt.show()

"""**Insights from Heat Map Correlation**

* Most features have weak or no correlation with each other (~0.00 to 0.04). these features don’t strongly influence each other linearly and are likely to bring unique information.

* Balance and NumOfProducts have the most notable (but still moderate) relationship but their correlation is not strong enough to cause concern like multicollinearity. They have a slight negative correlation (-0.30), as the number of products increases, balance decreases a bit.

"""

df.info()

"""***b. Analysis on Categorical data***"""

categorical_data = df.select_dtypes(include= 'object').columns.to_list()
categorical_data = ['HasCrCard'] + ['IsActiveMember'] + ['Exited'] + categorical_data

for col in categorical_data:
  plt.figure(figsize= (5,3))
  sns.countplot(data= df, x= col)
  plt.title(f'Distribution of {col}')
  plt.show()

"""There's an imbalance in the Exited field, which can affect the model performance.

Techniques will be applied like Class weighting and SMOTE to address this imbalance, and comparing each technique performance.

**4. Feature Engineering**
"""

X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
X

y

"""***a. One Hot Encoding on Categorical Features***"""

# One Hot Encoding the "Geography" and "Gender" column.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1,2])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
print(X)

"""***b. Train and Test Data***"""

# Splitting the dataset into the training and test data. Feature scaling is applied to only training set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

y_test

"""***c. Standardise the numerical features***"""

mask = (X < 0) | (X > 1)
cols_not_0_1 = np.any(mask, axis=0)
print(cols_not_0_1)
print("Columns with values not between 0 and 1:", np.where(cols_not_0_1)[0])
X_to_standardize = X[:, cols_not_0_1]

#Feature scaling
sc = StandardScaler()
X_train[:, cols_not_0_1] = sc.fit_transform(X_train[:, cols_not_0_1])
X_test[:, cols_not_0_1] = sc.transform(X_test[:, cols_not_0_1])

X_train

"""**5. Model Selection and Training**

The two models i'll be using are

*   LGBMClassifier
*   XGBClassifier

***a. Default parameters and Class Imbalance***

Training the models with default parameters and class imbalance
"""

# creating a dictionary for the models
from sklearn.ensemble import AdaBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

baseline_models = {
    'LGBMClassifier': LGBMClassifier(random_state= 42),
    'XGBClassifier': XGBClassifier(random_state= 42, use_label_encoder = False, eval_metric = 'logloss'),
}

"""Applying k-folds validation"""

# Applying k-fold validation on each model

from sklearn.model_selection import StratifiedKFold, cross_val_score
accuracy_scores = {}
precision_scores = {}
recall_scores = {}


for model_name, model in baseline_models.items():
  accuracy_scores[model_name] = cross_val_score(model, X_train, y_train, cv= 10 , scoring= 'accuracy')
  print(f'{model_name}: accuracy_mean {accuracy_scores[model_name].mean()}')
  print(f'{model_name}: accuracy_std {accuracy_scores[model_name].std()}')
  print(accuracy_scores[model_name])
  print('-'*50)

"""Testing the models with default parameters and class imbalance"""

for model_name, model in baseline_models.items():
  model.fit(X_train, y_train)
  baseline_model_pred = model.predict(X_test)

  print(model_name)
  print(f'Accuracy: \n', accuracy_score(y_test, baseline_model_pred))
  cm = confusion_matrix(y_test, baseline_model_pred)
  print("\nClassification Report:")
  print(classification_report(y_test, baseline_model_pred))
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
  xticklabels=["Not Churn", "Churn"],
  yticklabels=["Not Churn", "Churn"])
  plt.title(f"Confusion Matrix: {model_name}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()
  print('-'*70)

"""EVALUATION OF BASELINE MODELS

***LGBM MODEL***

 1. **Accuracy** :- The overall model **Accuracy** is 0.86, the model is able to correctly predict 86% of the total customers, whether they churn or not.

 2. **Precision Score** :-The model is able to **correctly predict 88%** of staying customers (class/label 0), and **correctly predict 74%** of customers who churn (class/label 1).

 3. **Recall Score** :- Out of all **customers who actually stay** (class/label 0), the model was able to **correctly identify and label 96%** of them, and out of all **customers who actually churn/leaves** (class/label 1), the model was able to **correctly identify and label 46%** of them.

 4. **F1-Score** :- The model has a high ability to recognise customers who stay (92%) and moderate ability to recognize customers who churn (57%)

 ***XGB MODEL***

 1. **Accuracy** :- The overall model **Accuracy** is 0.84, the model is able to correctly predict 84% of the total customers, whether they churn or not.

 2. **Precision Score** :-The model is able to **correctly predict 87%** of staying customers (class/label 0), and **correctly predict 64%** of customers who churn (class/label 1).

 3. **Recall Score** :- Out of all **customers who actually stay** (class/label 0), the model was able to **correctly identify and label 93%** of them, and out of all **customers who actually churn/leaves** (class/label 1), the model was able to **correctly identify and label 47%** of them.

 4. **F1-Score** :- The model has a high ability to recognise customers who stay (90%) and moderate ability to recognize customers who churn (54%).


**Conclusions**

 Both models have a high performance in predicting and identifying staying customers, however LGBM outperforms XGBoost in predicting leaving customers while XGBOOST slightly outperforms LGBM in identifying those customers who churn.

 The scores that need to improve are:

 ***LGBM***

 1. Recall score for class/label 1 (churn) = 46%
 2. F1-score for class/label 1 (churn) = 57%

 ***XGB***

 1. Precision score for class/label 1 (churn) = 64%
 2. Recall score for class/label 1 (churn) = 47%
 3. F1-score for class/label 1 (churn) = 54%

***b. Using Class Weighting and Default Parameters***

Class weight to balance the class imbalance for each model
"""

# ratio = number of negative / number of positive samples for XGBClassifier
ratio = np.sum(y_train == 0) / np.sum(y_train == 1)

# creating a dictionary for the models

models_class_weight = {
    'LGBMClassifier': LGBMClassifier(random_state= 42, class_weight = 'balanced'),
    'XGBClassifier': XGBClassifier(random_state= 42, scale_pos_weight = ratio, use_label_encoder = False, eval_metric = 'logloss')
}

from sklearn.utils import compute_sample_weight

for model_name, model in models_class_weight.items():
  model.fit(X_train, y_train)

  class_weight_pred = model.predict(X_test)


  print(model_name)
  print(f'Accuracy: \n', accuracy_score(y_test, class_weight_pred))
  cm = confusion_matrix(y_test, class_weight_pred)
  print("\nClassification Report:")
  print(classification_report(y_test, class_weight_pred))
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
  xticklabels=["Not Churn", "Churn"],
  yticklabels=["Not Churn", "Churn"])
  plt.title(f"Confusion Matrix: {model_name}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()
  print('-'*70)

"""EVALUATION OF CLASS WEIGHT MODEL & COMPARISON TO BASELINE MODEL  

***LGBM MODEL***

1. **Accuracy** :- There's a slight decrease of overall accuracy from 86% to 81%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 88% to 92%.

 2. **Recall Score** :- The model decreased significantly from 96% to 83%, although 83% is quite acceptable and a good score.

 3. **F1-Score** :- The model slightly decreases from 92% to 87%, still a good score and doesn't improvement.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a significant decrease from 74% to 52%, this needs improvement.

 2. **Recall Score** :- The model significantly increased from 46% to 71%.

 3. **F1-Score** :- The model slightly increases from 57% to 60%

***XGB MODEL***

 1. **Accuracy** :- There's a slight decrease of overall accuracy from 86% to 82%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 87% to 89%.

 2. **Recall Score** :- The model decreased from 93% to 87%, 87% is quite acceptable and a good score.

 3. **F1-Score** :- The model slightly decreases from 90% to 88%, still a good score and doesn't improvement.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a significant decrease from 64% to 54%, this needs improvement.

 2. **Recall Score** :- The model significantly increased from 47% to 58%.

 3. **F1-Score** :- The model slightly increases from 54% to 56%

**Conclusions**

 Both models still have a high performance in predicting and identifying staying customers, however LGBM now outperforms XGBoost in identifying leaving customers while XGBOOST slightly outperforms LGBM in predicting those customers who churn.

 The scores that need to improve are:

 ***LGBM***

 1. Precision score for class/label 1 (churn) = 52%
 2. F1-score for class/label 1 (churn) = 60%

 ***XGB***

 1. Precision score for class/label 1 (churn) = 54%
 2. Recall score for class/label 1 (churn) = 58%
 3. F1-score for class/label 1 (churn) = 56%

SYNTHETIC MINORITY OVERSAMPELLING TEchnique (SMOTE)

***c. SMOTE and Default Parameter***
"""

smote = SMOTE(random_state= 42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

unique_values, counts = np.unique(y_train_smote, return_counts=True)
print(unique_values)
print(counts)

y_train_smote

# creating a dictionary for the models

models_smote = {
    'LGBMClassifier': LGBMClassifier(random_state= 42),
    'XGBClassifier': XGBClassifier(random_state= 42, use_label_encoder = False, eval_metric = 'logloss')
}

for model_name, model in models_smote.items():
  model.fit(X_train_smote, y_train_smote)
  smote_pred = model.predict(X_test)

  print(model_name)
  print(f'Accuracy: \n', accuracy_score(y_test, smote_pred))
  cm = confusion_matrix(y_test, smote_pred)
  print("\nClassification Report:")
  print(classification_report(y_test, smote_pred))
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
  xticklabels=["Not Churn", "Churn"],
  yticklabels=["Not Churn", "Churn"])
  plt.title(f"Confusion Matrix: {model_name}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()
  print('-'*70)

"""EVALUATION OF SMOTE MODEL & COMPARISON TO BASELINE MODEL  

***LGBM MODEL***

1. **Accuracy** :- Accuracy remains stagnant at 86%

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 88% to 89%.

 2. **Recall Score** :- The model slightly decrease from 96% to 93%.

 3. **F1-Score** :- The model slightly decreases from 92% to 91%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a slight decrease from 74% to 69%. This outperforms the class weight model (52%).

 2. **Recall Score** :- The model significantly increased from 46% to 57%, although the class weight model performs better with a higher score of 71%.

 3. **F1-Score** :- The model slightly increases from 57% to 62%.

***XGB MODEL***

 1. **Accuracy** :- There's a slight decrease of overall accuracy from 86% to 84%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 87% to 89%.

 2. **Recall Score** :- The model decreased from 93% to 92%.

 3. **F1-Score** :- The model remains stagnant at 90%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- The model remains stagnant at 64%.

 2. **Recall Score** :- The model significantly increased from 47% to 53%.

 3. **F1-Score** :- The model slightly increases from 54% to 58%

**Conclusions**

 Both models still have a high performance in predicting and identifying staying customers. LGBM and XGBoost performs better when SMOTE is applied on both models compared to balancing the class with class weighting.
 The scores that need to improve are:

 ***LGBM***

 1. Recall score for class/label 1 (churn) = 57%
 2. F1-score for class/label 1 (churn) = 62%

 ***XGB***

 1. Recall score for class/label 1 (churn) = 53%
 2. F1-score for class/label 1 (churn) = 58%

***d. Class Weight and HyperParameter Tuning (improving f1 score)***
"""

param_grids = {
    'LGBMClassifier': {
        'num_leaves': [20, 31, 50, 100],
        'max_depth': [-1, 5, 10, 15],
        'learning_rate': [0.01, 0.05, 0.1],
        'n_estimators': [50, 100, 200,300],
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0, 0.1, 0.5]
    },
    'XGBClassifier': {
        'n_estimators': [50 ,100, 200,300],
        'learning_rate': [0.01, 0.05, 0.1, 0.2],
        'max_depth': [3, 5, 7, 10,16],
        'min_child_weight': [1, 3, 5],
        'gamma': [0, 0.1, 0.5, 1],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0],
        'reg_alpha': [0, 0.1, 0.5],
        'reg_lambda': [0.5, 1, 1.5]
    }

}

# Perform RandomizedSearchCV for each model using its specific parameter grid
for model_name, model in models_class_weight.items():
    # Get the correct parameter grid for the current model
  parameter = param_grids[model_name]

  # Use RandomizedSearchCV with the specific parameter grid
  search = RandomizedSearchCV(estimator=model, param_distributions=parameter, n_iter=50, cv=5, scoring='f1', n_jobs=-1)
  search.fit(X_train, y_train)

  best_params = search.best_params_
  best_score = search.best_score_
  best_model = search.best_estimator_

  tuned_class_weight = best_model.predict(X_test)
  cm = confusion_matrix(y_test, tuned_class_weight)

  # Print the results
  print(f'Best Parameters for {model_name}: {best_params}')
  print(f'Best Score for {model_name}: {best_score}')
  print(f'Best Estimator for {model_name}: {best_model}')
  print("Confusion Matrix:")
  print(confusion_matrix(y_test, tuned_class_weight))
  print("\nClassification Report:")
  print(classification_report(y_test, tuned_class_weight))
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Not Churn", "Churn"],
                yticklabels=["Not Churn", "Churn"])
  plt.title(f"Confusion Matrix: {model_name}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()
  print('-'*50)

"""EVALUATION OF TUNED MODELS & CLASS WEIGHT

The aim of applying class weight was to address the class imbalance in order to improve the F1 and Recall scores.

***LGBM MODEL***

1. **Accuracy** :- Accuracy decreases from 86% to 81%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 88% to 91%.

 2. **Recall Score** :- The model slightly decrease from 96% to 84%.

 3. **F1-Score** :- The model slightly decreases from 92% to 88%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a significant decrease from 74% to 53%.

 2. **Recall Score** :- The model significantly increased from 46% to 69%.

 3. **F1-Score** :- The model slightly increases from 57% to 60%.

***XGB MODEL***

 1. **Accuracy** :- There's a slight decrease of overall accuracy from 86% to 82%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 87% to 91%.

 2. **Recall Score** :- The model decreased from 93% to 86%.

 3. **F1-Score** :- There's a slight decrease from 90% to 89%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a slight decrease from 64% to 56%.

 2. **Recall Score** :- The model significantly increased from 47% to 68%.

 3. **F1-Score** :- The model increases from 54% to 61%.

**Conclusions**

 Both models still have a high performance in predicting and identifying staying customers. LGBM performs slighlty better with the hyperparameters applied.

***e. SMOTE and HyperParameter Tuning***

Using the same HyperParameter as with Class Weight
"""

# Perform RandomizedSearchCV for each model using its specific parameter grid
for model_name, model in models_smote.items():

  # Get the correct parameter grid for the current model
  parameter = param_grids[model_name]

  # Handle XGBoost specific parameter

  # Use RandomizedSearchCV with the specific parameter grid
  search_smote = RandomizedSearchCV(estimator= model, param_distributions=parameter, n_iter=50, cv=5, scoring='f1', n_jobs=-1, verbose=1)

  search_smote.fit(X_train_smote, y_train_smote)
    #search.fit(X_train, y_train)

  best_params_smote = search_smote.best_params_
  best_score_smote = search_smote.best_score_
  best_model_smote = search_smote.best_estimator_

  smote_grid_pred = best_model_smote.predict(X_test)
  cm = confusion_matrix(y_test, smote_grid_pred)

    # Print the results
  print(f'Best Parameters for {model_name}: {best_params_smote}')
  print(f'Best Score for {model_name}: {best_score_smote}')
  print(f'Best Estimator for {model_name}: {best_model_smote}')
  print("Confusion Matrix:")
  print(confusion_matrix(y_test, smote_grid_pred))
  print("\nClassification Report:")
  print(classification_report(y_test, smote_grid_pred))
  plt.figure(figsize=(6, 5))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Not Churn", "Churn"],
                yticklabels=["Not Churn", "Churn"])
  plt.title(f"Confusion Matrix: {model_name}")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()
  print('-'*50)

"""EVALUATION OF TUNED MODELS & SMOTE

The aim of applying class weight was to address the class imbalance in order to improve the F1 and Recall scores.

***LGBM MODEL***

1. **Accuracy** :- There's a slight decrease from 86% to 85%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's no change, remains at 88%.

 2. **Recall Score** :- The model slightly decrease from 96% to 94%.

 3. **F1-Score** :- The model slightly decreases from 92% to 91%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a significant decrease from 74% to 68%.

 2. **Recall Score** :- The model slightly increased from 46% to 52%.

 3. **F1-Score** :- The model remains stagnant at 59%.

***XGB MODEL***

 1. **Accuracy** :- There's a slight decrease of overall accuracy from 86% to 84%.

         Class 0 - Customers who stays


 1. **Precision Score** :-There's a slight increase from 87% to 88%.

 2. **Recall Score** :- The model slightly icreased from 93% to 92%.

 3. **F1-Score** :- The model remains stagnant at 90%.

         Class 1 - Customers who leaves


 1. **Precision Score** :- There's a slight decrease from 64% to 63%.

 2. **Recall Score** :- The model slightly increased from 47% to 52%.

 3. **F1-Score** :- The model slightly increases from 54% to 57%.

**Conclusions**

 Both models still have a high performance in predicting and identifying staying customers. They're both good at predicting churned customers (precision) but not classifying them (recall).

**6.Model Comparisons**

The models to be compared are as follows

* Baseline LGBM Model

* Baseline XGBoost Model

* LGBM + Class Weighting

* XGBoost + Class Weighting

* LGBM + SMOTE

* XGBoost + SMOTE

* Tuned LGBM + Class Weighting

* Tuned XGBoost + Class Weighting

* Tuned LGBM + SMOTE

* Tuned XGBoost + SMOTE

   

**a. BASELINE LGBM MODEL**

***Confusion Matrix***

       [[1528  65]

       [218   189]]

***Classification Report: (Overall accuracy 0.86)***

              precision    recall  f1-score   support

           0       0.88      0.96      0.92      1593
           1       0.74      0.46      0.57       407

This model is very good in predicting customers who churn but not quite good in identifying those customers that do churn. It is not a strong model in balancing Precision and Recall for churn detection. It has less False Positives and more False Negative. It predicts more churners as not-churned which can be costly to the business and lead to revenue loss.

**b. BASELINE XGBOOST MODEL**

***Confusion Matrix***

      [[1483  110]
      [214  193]]

***Classification Report: (Overall accuracy is 0.84)***

              precision    recall  f1-score   support

           0       0.87      0.93      0.90      1593
           1       0.64      0.47      0.54       407

This model is outperformed by the Baseline LGBM Model in predicting churned customers, and has less balance in precisison and recall. Still a  high number of False Negative, but slightly better than the LGBM Model.

**c. LGBM MODEL + CLASS WEIGHTING**

***Confusion Matrix***

      [[1329  264]
      [117  290]]

***Classification Report: (Overall accuracy 0.81)***

              precision    recall  f1-score   support

           0       0.92      0.83      0.87      1593
           1       0.52      0.71      0.60       407

This model is strong in balancing Precision and Recall for the class/label 1. The number of False Positives and True Positives in this model are significantly higher than the baseline models, but significantly less False Negatives, making this model suitable if retention cost is not high, that is the cost of retaining and keeping existing customers who leave is not high (i.e., discount, offers, calls). Lower False Negatives comes with a trade-off of higher False Negatives.

**d. XGBOOST MODEL + CLASS WEIGHTING**

***Confusion Matrix***

      [[1393  200]
      [169  238]]

***Classification Report: (Overall accuracy 0.82)***

              precision    recall  f1-score   support

           0       0.89      0.87      0.88      1593
           1       0.54      0.58      0.56       407

The LGBM Model + class weighting outperforms this model, it is not quite robust in balancing Precision and Recall. There's higher False Negatives  compared to the LGBM + Class Weighting Model.

**e. LGBM MODEL + SMOTE**

***Confusion Matrix***

      [[1489  104]
      [176  231]]

***Classification Report: (Overall accuracy 0.86)***

              precision    recall  f1-score   support

           0       0.89      0.93      0.91      1593
           1       0.69      0.57      0.62       407

This model is more robust in balancing Precision and Recall for the class/label 1, although there are higher False Negatives than False Positives, which means there's more risk of losing a customer which can lead to revenue loss.

**f. XGBOOST MODEL + SMOTE**

***Confusion Matrix***

      [[1471  122]
      [191  216]]

***Classification Report: (Overall accuracy 0.84)***

              precision    recall  f1-score   support

           0       0.89      0.92      0.90      1593
           1       0.64      0.53      0.58       407

This Model does not have a good performance, there's higher False Negatives which is costly to the business, making this model not idea for the business problem.

**g. TUNED LGBM + CLASS WEIGHTING**

***Confusion Matrix***

       [[1340  253]
       [127  280]]

***Classification Report: (Overall accuracy 0.81)***

              precision    recall  f1-score   support

           0       0.91      0.84      0.88      1593
           1       0.53      0.69      0.60       407

This model performs quite well in balancing Precision and Recall, however the number of False Negatives is a little higher than the LGBM + Class Weight Model. These little higher FN comes with a trade-off of lower False Positives, which can be reduce retention cost and raising less false alarms while also preventing high revenue loss.
  

**g. TUNED XGBOOST + CLASS WEIGHTING**

***Confusion Matrix***

       [[1374  219]
       [ 132  275]]

***Classification Report: (Overall accuracy 0.81)***
              precision    recall  f1-score   support

           0       0.91      0.86      0.89      1593
           1       0.56      0.68      0.61       407

This model is robust in balancing Precision and Recall, however the number of False Negatives is a little higher than the LGBM + Class Weight Model and Tuned LGBM + Class Weight Models. These little higher FN comes with a trade-off of lower False Positives which is tolerable for some business. This trade-off is moderate as the number of False Negatives has not incresed significantly, yet it has significantly reduced the False Positives.

**h. TUNED LGBM & SMOTE**

***Confusion Matrix:***

        [[1494   99]
        [208  199]]

***Classification Report: (Overall accuracy 0.85)***

              precision    recall  f1-score   support

           0       0.88      0.94      0.91      1593
           1       0.67      0.49      0.56       407

This Model does not have a good performance, there's high False Negatives and less True Positives.

**TUNED XGBOOST & SMOTE**

***Confusion Matrix:***

       [[1484  109]
       [ 194  213]]

***Classification Report: (Overall accuracy 0.85)***

              precision    recall  f1-score   support

           0       0.88      0.93      0.91      1593
           1       0.66      0.52      0.58       407

This Model does not have a good performance, there's high False Negatives and less True Positives.

**6. Model Selection**

For this project, i will use the Tuned XGBOOST + Class Weight Model because of the balance it brings between the False Positives and False Negatives. If the business prioritise catching more churners with the risk of high rate of false alarms, the LGBM + Class Weight is recommended. The selected model for this project significantly lowers the rate of false alarms by slightly increasing false negatives, this trade-off is acceptable, as it reduces retention waste while still catching a significant amount of churners.
"""

Best_Parameter_XGBoost= {'subsample': 0.7, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 100, 'min_child_weight': 3, 'max_depth': 10,
                         'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.8}
# ratio = number of negative / number of positive samples for XGBClassifier
ratio = np.sum(y_train == 0) / np.sum(y_train == 1)

# creating a dictionary for the models
selected_model = XGBClassifier(random_state= 42, scale_pos_weight = ratio, use_label_encoder = False, eval_metric = 'logloss', subsample = 0.7,
                               reg_lambda = 1, reg_alpha = 0, n_estimators = 100, min_child_weight = 3, max_depth = 10, learning_rate = 0.01,
                               gamma = 0.1, colsample_bytree = 0.8)

best_model = selected_model.fit(X_train, y_train)

best_model_pred = best_model.predict(X_test)

# Classification Repor
print("XGBOOST Performance:")
print(classification_report(y_test, best_model_pred))
print('-'*50)

# Confusion Matrix
print("XGBOOST Confusion Matrix:")
print(confusion_matrix(y_test, best_model_pred))
print('-'*50)

"""**PR Curve Analysis for Selected Model**

The PR AUC (Precision Recall - Area Under Curve) measures the model's performance on the positove class (Churned customers). This is especially important with imbalance data like the bank customer dataset. It plots precision against Recall.
"""

from sklearn.metrics import precision_recall_curve, average_precision_score

xgb_pred = best_model.predict_proba(X_test)[:, 1]

# PR AUC
print("XGBOOST PR AUC:", round(average_precision_score(y_test, xgb_pred),2))

# Plot
prec_xgb, rec_xgb, _ = precision_recall_curve(y_test, xgb_pred)
plt.plot(rec_xgb, prec_xgb, label="XGBOOST")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()

"""PR AUC Score	Interpretation

0.90 – 1.00	Excellent – high precision and recall

0.75 – 0.90	Very good

0.60 – 0.75	Good – solid performance

0.50 – 0.60	Moderate – room for improvement

< 0.50	    Poor – may miss many positives or misclassify

Based on the PR curve above, the PR-AUC score obtained 0.69, which falls under the “Good - Solid Performance” category.

The score indicates that the Model has a good and solid performance on predicting positive cases correctly at various levels of recall.

**ROC Curve Analysis for Selected Model**

The ROC AUC (Receiver Operating Characteristic - Area Under Curve) measures the model's ability to distinguish between the positive and negative classes across all thresholds. it plots the True Positive Rate against the False positive Rate.
"""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Calculate ROC curve
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb_pred)

# Calculate ROC-AUC score
roc_auc_xgb = roc_auc_score(y_test, xgb_pred)

# Print ROC-AUC Score
print('ROC-AUC Score:', roc_auc_xgb)

# Plot ROC curve
plt.figure()
plt.plot(fpr_xgb, tpr_xgb, color='darkblue', lw=2, label=f'ROC curve XGBOOST (area = {roc_auc_xgb:0.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""**ROC-AUC Interpretation**

   0.9-1.0 --> Excellent or outstanding

   0.8-0.9 --> Very good

   0.7-0.8 --> Good

   0.6-0.7 --> Fair

   0.5-0.6 --> Poor

   0.5 --> No better than random

   < 0.5 --> Worse than random

Based on the ROC curve above, the ROC-AUC score obtained 0.85, which falls under the “Very Good” category.

The score indicates that the Model has strong discriminative power and performs well in distinguishing between churn and non-churn customers.

**SAVING THE SELECTED MODEL**
"""

# Save the trained model into the file
import joblib
joblib.dump(selected_model, "selected_model_xgb.pkl")
print("Best Model (XGBOOST) has been saved as selected_model_xgb.pkl")

"""**Predicting on New Customer Data**"""

new_customer_example = pd.DataFrame([{
       'CreditScore': 600,
    'Geography': 'Spain', # Use original category
    'Gender': 'Male',     # Use original category
    'Age': 25,
    'Tenure': 3,
    'Balance': 90000,
    'NumOfProducts': 2,
    'HasCrCard': 1,
    'IsActiveMember': 1,
    'EstimatedSalary': 750000
}])

new_customer_transformed = ct.transform(new_customer_example.values)

# Find the columns that were standardized during training
# This mask was created earlier in the notebook
mask = (X < 0) | (X > 1)
cols_not_0_1 = np.any(mask, axis=0)
new_customer_transformed[:, cols_not_0_1] = sc.transform(new_customer_transformed[:, cols_not_0_1])

# Load model
churn_model = joblib.load("selected_model_xgb.pkl")

churn_prob = churn_model.predict_proba(new_customer_transformed)

churn_pred = churn_model.predict(new_customer_transformed)

label = "Churn" if churn_pred.any == 1 else "Not Churn" # If prediction [0] is 1, it means Churn, if the result is 0 then it means Not Churn
print(f"Prediction: {label}") # Show the prediction result as Churn or Not Churn
print(f'Classes:    {churn_model.classes_}')
print(f"Probability: {churn_prob[0]}") # Show the probability of Churn and Not Churn

#Ranking System to catch categories of chunners
if churn_prob[ : ,1] >= 0.70:
  print('High Risk of Churn')
elif churn_prob[ : ,1] >= 0.40:
  print('Medium Risk of Churn')
else:
  print('Low Risk of Churn')

"""**STREAMLIT APP**

The model has been uploaded to the streamlit app to enable customers make prediction and see ranking of potential chunners with new data.

To access the app, please use the link below.

[Streamlit App](https://bank-customer-churn-mbiupxnckhdxrrgncv76f4.streamlit.app/#bank-customer-churn-prediction-app)

"""

